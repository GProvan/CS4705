{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmprovan/CS4705/blob/main/CS4705_assignment_1_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aim of this assignment is to provide a means to use the tools introduced in class.\n",
        "We will thus perform data preprocessing and compare decision tree and probabilistic classifiers. I provide you with helper code at the end of the assignment description. The marking for each question is shown.\n",
        "\n",
        "\n",
        "\n",
        "Data: Census income data set\n",
        "\n",
        "https://www.kaggle.com/uciml/adult-census-income\n",
        "\n",
        "1. 30%: Run the following data imputation methods:\n",
        "\n",
        "  -Mean Imputation\n",
        "  \n",
        "  -Cold Deck Imputation\n",
        "  \n",
        "  -Regression Imputation\n",
        "\n",
        "2.  10%: Split the data into training and test subsets (80/20 split)\n",
        "\n",
        "3. 30%: Compare the predictive accuracy of following classifiers; use standard statistical testing, e.g., p-tests, etc.\n",
        "\n",
        "  -Decision Tree\n",
        "  \n",
        "  -Decision forest\n",
        "  \n",
        "  -naive Bayes (multinomial and Gaussian)\n",
        "  \n",
        "  -Regression\n",
        "\n",
        "4. 30%: Comment on the \"quality\" of the different imputation methods and on the predictive accuracy of the classifiers; use standard statistical testing, e.g., p-tests, etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tLxP2ymdGjLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPUTATION METHODS\n",
        "\n",
        "**Mean Imputation**\n",
        "\n",
        "Replace the missing value of the observation with a randomly selected value from all the observations in the sample that has similar values on other variables.\n",
        "\n",
        "Thus, this technique ensures that the imputing value is only selected from the possible interval where the actual value could probably fall, and is randomly selected rather than being determined, which is an essential aspect for a correct standard error.\n",
        "\n",
        "**Randomized Imputation**\n",
        "\n",
        "Replace the missing data using a value chosen randomly.\n",
        "\n",
        "**Hot Deck Imputation**\n",
        "\n",
        "Replace the missing data using a value chosen from other variables with similar observation values in this technique.\n",
        "\n",
        "**Regression Imputation**\n",
        "\n",
        "Regression imputation involves fitting a regression model on a feature with missing data and then using this regression model’s predictions to replace the missing values in this feature. This technique preserves the relationships between features, and this grants it a significant advantage over simple imputation techniques such as mean and mode imputation.\n",
        "\n",
        "Regression imputation can be defined into two categories:\n",
        "Deterministic regression imputation\n",
        "\n",
        "a) Deterministic regression imputation:\n",
        "\n",
        "imputes the missing data with the exact value predicted from the regression model. This technique doesn’t consider the random variation around the regression line. Since the imputed values are exact, the correlation between the features and the dependent variables is overestimated.\n",
        "\n",
        "\n",
        "b) Stochastic regression imputation\n",
        "\n",
        "In stochastic regression imputation, we add a random variation (error term) to the predicted value, therefore, reproducing the correlation of X and Y more appropriately."
      ],
      "metadata": {
        "id": "XV6sDqlWGjLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HELPER CODE IS SHOWN BELOW"
      ],
      "metadata": {
        "id": "t9ZTUiOeGjLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import permutation_test_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score"
      ],
      "metadata": {
        "id": "5emua4AMGjLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download the data and then load it\n",
        "org_data = pd.read_csv('./adult.csv', header=0)\n",
        "org_data.shape"
      ],
      "metadata": {
        "id": "6JqrJfjdGjLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-process the data; visualise the data\n",
        "replace ? with nan"
      ],
      "metadata": {
        "id": "8-Jjx1HJGjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "org_data[org_data == \"?\"] = np.nan"
      ],
      "metadata": {
        "id": "7N_oO2WbGjLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation phase"
      ],
      "metadata": {
        "id": "tLZH-35nGjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the class called SimpleImputer from impute model in sklearn\n",
        "from sklearn.impute import SimpleImputer\n",
        "# To replace the missing value we create below object of SimpleImputer class\n",
        "imputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "''' Using the fit method, we apply the `imputa` object on the matrix of our feature x.\n",
        "The `fit()` method identifies the missing values and computes the mean of such feature a missing value is present.\n",
        "'''\n",
        "imputa.fit(x[:, 1:3])\n",
        "# Repalcing the missing value using transform method\n",
        "x[:, 1:3] = imputa.transform(x[:, 1:3])\n",
        "\n"
      ],
      "metadata": {
        "id": "dH3ctm0OGjLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split data into train and test"
      ],
      "metadata": {
        "id": "82voBOHGGjLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state= 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "qI9BoKXbGjLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "scaling of features"
      ],
      "metadata": {
        "id": "ZCL5kLm3GjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "# we only aply the feature scaling on the features other than dummy variables.\n",
        "x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n",
        "x_test[:, 3:] = sc.fit_transform(x_test[:, 3:])\n",
        "\n"
      ],
      "metadata": {
        "id": "SaXWPW4iGjLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train classifiers"
      ],
      "metadata": {
        "id": "TR9CRETUGjLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#classifiers: naive Bayes as an example\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "\n",
        "nb.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred_nb = nb.predict(X_test)"
      ],
      "metadata": {
        "id": "w6mfzTueGjLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide statistical tests to compare the classifiers and the impact of imputation types"
      ],
      "metadata": {
        "id": "7iSdmKWyGjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "leDC3Gg0GjLW"
      }
    }
  ]
}