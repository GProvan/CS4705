{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The aim of this assignment is to provide a means to use the tools introduced in class. \nWe will thus perform data preprocessing and compare decision tree and probabilistic classifiers. I provide you with helper code at the end of the assignment description. The marking for each question is shown.\n\n \n\nData: Census income data set\n\nhttps://www.kaggle.com/uciml/adult-census-income\n\n1. 30%: Run the following data imputation methods:\n\n  -Mean Imputation\n  \n  -Cold Deck Imputation\n  \n  -Regression Imputation\n\n2.  10%: Split the data into training and test subsets (80/20 split)\n\n3. 30%: Compare the predictive accuracy of following classifiers; use standard statistical testing, e.g., p-tests, etc.\n\n  -Decision Tree\n  \n  -Decision forest\n  \n  -naive Bayes (multinomial and Gaussian)\n  \n  -Regression\n\n4. 30%: Comment on the \"quality\" of the different imputation methods and on the predictive accuracy of the classifiers; use standard statistical testing, e.g., p-tests, etc.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"IMPUTATION METHODS\n\nMean Imputation\n\nReplace the missing value of the observation with a randomly selected value from all the observations in the sample that has similar values on other variables.\n\nThus, this technique ensures that the imputing value is only selected from the possible interval where the actual value could probably fall, and is randomly selected rather than being determined, which is an essential aspect for a correct standard error.\n\nCold Deck Imputation\n\nReplace the missing data using a value chosen from other variables with similar observation values in this technique. The difference between this technique and the Hot Deck imputation is that the selecting process of the imputing value is not randomized.\n\nRegression Imputation\n\nRegression imputation involves fitting a regression model on a feature with missing data and then using this regression model’s predictions to replace the missing values in this feature. This technique preserves the relationships between features, and this grants it a significant advantage over simple imputation techniques such as mean and mode imputation.\n\nRegression imputation can be defined into two categories:\nDeterministic regression imputation\n\na) Deterministic regression imputation:\n\nimputes the missing data with the exact value predicted from the regression model. This technique doesn’t consider the random variation around the regression line. Since the imputed values are exact, the correlation between the features and the dependent variables is overestimated.\n\n\nb) Stochastic regression imputation\n\nIn stochastic regression imputation, we add a random variation (error term) to the predicted value, therefore, reproducing the correlation of X and Y more appropriately.","metadata":{}},{"cell_type":"markdown","source":"HELPER CODE IS SHOWN BELOW","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#download the data and then load it\norg_data = pd.read_csv('./adult.csv', header=0)\norg_data.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pre-process the data; visualise the data\nreplace ? with nan","metadata":{}},{"cell_type":"code","source":"org_data[org_data == \"?\"] = np.nan","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Imputation phase","metadata":{}},{"cell_type":"code","source":"# Importing the class called SimpleImputer from impute model in sklearn\nfrom sklearn.impute import SimpleImputer\n# To replace the missing value we create below object of SimpleImputer class\nimputa = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n''' Using the fit method, we apply the `imputa` object on the matrix of our feature x.\nThe `fit()` method identifies the missing values and computes the mean of such feature a missing value is present.\n'''\nimputa.fit(x[:, 1:3])\n# Repalcing the missing value using transform method\nx[:, 1:3] = imputa.transform(x[:, 1:3])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split data into train and test","metadata":{}},{"cell_type":"code","source":"#split data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state= 1)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scaling of features","metadata":{}},{"cell_type":"code","source":"#feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# we only aply the feature scaling on the features other than dummy variables.\nx_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\nx_test[:, 3:] = sc.fit_transform(x_test[:, 3:])\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train classifiers","metadata":{}},{"cell_type":"code","source":"#classifiers: naive Bayes as an example\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\n\nnb.fit(X_train, Y_train)\n\nY_pred_nb = nb.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Provide statistical tests to compare the classifiers and the impact of imputation types","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}